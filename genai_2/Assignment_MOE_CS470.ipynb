{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2bd7eb17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bd7eb17",
        "outputId": "0c0907a6-e122-47e3-8a17-e085adf9c433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
        "%pip install groq python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a FREE, open-source model from Hugging Face\n",
        "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6c67eb1c",
      "metadata": {
        "id": "6c67eb1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8aa675b-93b9-4cca-a003-bdd6c687fefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Securely accept API key (hidden while typing)\n",
        "GROQ_API_KEY = getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "58dd1396",
      "metadata": {
        "id": "58dd1396"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG = {\n",
        "    \"technical\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are a highly skilled Technical Support Engineer. \"\n",
        "            \"You specialize in debugging code, analyzing error messages, and providing \"\n",
        "            \"precise, step-by-step technical solutions. Be rigorous, code-focused, and exact. \"\n",
        "            \"When relevant, provide corrected code snippets.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7,\n",
        "    },\n",
        "    \"billing\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are an empathetic Billing Support Specialist. \"\n",
        "            \"You handle refund requests, subscription issues, and payment disputes with care. \"\n",
        "            \"Be understanding, policy-driven, and clear about next steps. \"\n",
        "            \"Always acknowledge the customer's frustration before explaining the resolution process.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7,\n",
        "    },\n",
        "    \"general\": {\n",
        "        \"system_prompt\": (\n",
        "            \"You are a friendly and helpful General Support Assistant. \"\n",
        "            \"You handle casual inquiries, FAQs, and any questions that don't fall under \"\n",
        "            \"technical or billing categories. Be warm, concise, and helpful.\"\n",
        "        ),\n",
        "        \"temperature\": 0.7,\n",
        "    },\n",
        "}\n",
        "\n",
        "MODEL_NAME = \"llama-3.3-70b-versatile\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4dcd7e45",
      "metadata": {
        "id": "4dcd7e45"
      },
      "outputs": [],
      "source": [
        "def route_prompt(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Classifies user input into one of: 'technical', 'billing', or 'general'.\n",
        "    Uses temperature=0 for consistent, deterministic classification.\n",
        "    Returns ONLY the category name as a lowercase string.\n",
        "    \"\"\"\n",
        "    routing_system_prompt = (\n",
        "        \"You are an intent classification engine. \"\n",
        "        \"Classify the user's message into exactly one of these categories: \"\n",
        "        \"technical, billing, general. \"\n",
        "        \"Rules:\\n\"\n",
        "        \"- 'technical': bugs, errors, code issues, crashes, API problems\\n\"\n",
        "        \"- 'billing': charges, refunds, subscriptions, payments, invoices\\n\"\n",
        "        \"- 'general': everything else\\n\"\n",
        "        \"Return ONLY the single category word in lowercase. No punctuation, no explanation.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": routing_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        temperature=0,  # Deterministic for routing\n",
        "        max_tokens=10,\n",
        "    )\n",
        "\n",
        "    category = response.choices[0].message.content.strip().lower()\n",
        "\n",
        "    # Fallback to 'general' if unexpected output\n",
        "    if category not in MODEL_CONFIG:\n",
        "        category = \"general\"\n",
        "\n",
        "    return category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "234faff9",
      "metadata": {
        "id": "234faff9"
      },
      "outputs": [],
      "source": [
        "def process_request(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the full MoE pipeline:\n",
        "    1. Routes the input to the correct expert category.\n",
        "    2. Loads the corresponding system prompt.\n",
        "    3. Calls the LLM with that expert configuration.\n",
        "    4. Returns the expert's response.\n",
        "    \"\"\"\n",
        "    # Step 1: Route to the right expert\n",
        "    category = route_prompt(user_input)\n",
        "    print(f\"[Router] Classified as: '{category}'\")\n",
        "\n",
        "    # Step 2: Select expert config\n",
        "    expert_config = MODEL_CONFIG[category]\n",
        "\n",
        "    # Step 3: Call the LLM with the expert's system prompt\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": expert_config[\"system_prompt\"]},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        temperature=expert_config[\"temperature\"],\n",
        "        max_tokens=512,\n",
        "    )\n",
        "\n",
        "    # Step 4: Return the expert's answer\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c1e1581d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1e1581d",
        "outputId": "a0fbcae3-5e13-45ba-b126-4a5b88cfa504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "User Query: My python script is throwing an IndexError on line 5.\n",
            "============================================================\n",
            "[Router] Classified as: 'technical'\n",
            "\n",
            "[Expert Response]\n",
            "To assist you with the `IndexError` on line 5 of your Python script, I'll need more information about your code. However, I can guide you through a general troubleshooting process.\n",
            "\n",
            "### Understanding IndexError\n",
            "An `IndexError` typically occurs when you try to access an element in a list (or other sequence type) using an index that is out of range. For example, if you have a list with 3 elements (indices 0, 1, and 2), trying to access the element at index 3 would result in an `IndexError`.\n",
            "\n",
            "### General Steps to Debug\n",
            "1. **Check the Line of Code**: Look at line 5 of your script and identify the operation that's causing the error. It's likely an indexing operation on a list, tuple, or string.\n",
            "2. **Variable Inspection**: Before line 5, add some debug print statements to inspect the variables involved in the indexing operation. This can help you understand the length and contents of the sequence you're trying to access.\n",
            "3. **Index Validation**: Make sure the index you're trying to access is within the valid range for the sequence. Remember, Python uses 0-based indexing, so the last valid index for a sequence of length `n` is `n-1`.\n",
            "\n",
            "### Example Error and Solution\n",
            "Let's say your line 5 looks something like this:\n",
            "\n",
            "```python\n",
            "my_list = [1, 2, 3]\n",
            "print(my_list[3])  # This would throw an IndexError\n",
            "```\n",
            "\n",
            "To fix this, you need to ensure the index is within the bounds of `my_list`. You can do this by checking the length of `my_list` before trying to access an element:\n",
            "\n",
            "```python\n",
            "my_list = [1, 2, 3]\n",
            "index = 3\n",
            "if index < len(my_list):\n",
            "    print(my_list[index])\n",
            "else:\n",
            "    print(f\"Index {index} is out of range for my_list\")\n",
            "```\n",
            "\n",
            "### Request for More Information\n",
            "To provide a more specific solution, could you please share:\n",
            "- The exact code on line 5 of your script?\n",
            "- Any relevant code leading up to line 5 that might affect the variables involved?\n",
            "- The full error message you're seeing, including any traceback information?\n",
            "\n",
            "This will allow me to offer a more precise and tailored solution to your problem.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_1 = \"My python script is throwing an IndexError on line 5.\"\n",
        "print(\"=\" * 60)\n",
        "print(f\"User Query: {query_1}\")\n",
        "print(\"=\" * 60)\n",
        "answer_1 = process_request(query_1)\n",
        "print(f\"\\n[Expert Response]\\n{answer_1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "67a7a5ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a7a5ab",
        "outputId": "656c4951-bc44-4d8f-c304-d766dc008367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "User Query: I was charged twice for my subscription this month.\n",
            "============================================================\n",
            "[Router] Classified as: 'billing'\n",
            "\n",
            "[Expert Response]\n",
            "I'm so sorry to hear that you were charged twice for your subscription this month. I can imagine how frustrating that must be for you, especially when you're expecting to only see one charge on your statement. I'm here to help you resolve this issue as quickly and fairly as possible.\n",
            "\n",
            "To get started, I'll need to look into your account and verify the duplicate charge. Can you please confirm your subscription details, such as the type of subscription you have and the date you were charged? Additionally, I may need to ask for some additional information to ensure that I'm investigating the correct transaction.\n",
            "\n",
            "Once I've verified the duplicate charge, I'll work with our team to process a refund for the incorrect charge as soon as possible. Please know that we have a standard refund process in place, and I'll guide you through the next steps. If you have any questions or concerns, please don't hesitate to ask. Your satisfaction is my top priority, and I'm committed to making things right.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_2 = \"I was charged twice for my subscription this month.\"\n",
        "print(\"=\" * 60)\n",
        "print(f\"User Query: {query_2}\")\n",
        "print(\"=\" * 60)\n",
        "answer_2 = process_request(query_2)\n",
        "print(f\"\\n[Expert Response]\\n{answer_2}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cc999db8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc999db8",
        "outputId": "b5beaa7e-a5f9-499f-9893-925920456bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "User Query: What are your business hours?\n",
            "============================================================\n",
            "[Router] Classified as: 'general'\n",
            "\n",
            "[Expert Response]\n",
            "Our business hours are Monday to Friday, 9:00 AM to 5:00 PM (EST). We're closed on weekends and major holidays. If you have any questions or need assistance outside of these hours, feel free to leave a message, and we'll get back to you as soon as possible during our next business day. How can I assist you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_3 = \"What are your business hours?\"\n",
        "print(\"=\" * 60)\n",
        "print(f\"User Query: {query_3}\")\n",
        "print(\"=\" * 60)\n",
        "answer_3 = process_request(query_3)\n",
        "print(f\"\\n[Expert Response]\\n{answer_3}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BONUS"
      ],
      "metadata": {
        "id": "SJvBJr5Y_1nJ"
      },
      "id": "SJvBJr5Y_1nJ"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "847ac87f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847ac87f",
        "outputId": "8778e75b-7e02-4002-b1fc-f609843ba047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "User Query: What is the current price of Bitcoin?\n",
            "============================================================\n",
            "[Router] Classified as: 'tool'\n",
            "\n",
            "[Expert Response]\n",
            "The current price of Bitcoin (BTC) is $67,432.15 USD (mock data).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_mock_bitcoin_price() -> str:\n",
        "    \"\"\"Mock function simulating a live data fetch for Bitcoin price.\"\"\"\n",
        "    # In a real scenario, this would call an API like CoinGecko\n",
        "    mock_price = 67_432.15\n",
        "    return f\"The current price of Bitcoin (BTC) is ${mock_price:,.2f} USD (mock data).\"\n",
        "\n",
        "\n",
        "def route_prompt_with_tool(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Extended router that also detects 'tool' intent for live data queries.\n",
        "    Returns one of: 'technical', 'billing', 'general', 'tool'.\n",
        "    \"\"\"\n",
        "    routing_system_prompt = (\n",
        "        \"You are an intent classification engine. \"\n",
        "        \"Classify the user's message into exactly one of these categories: \"\n",
        "        \"technical, billing, general, tool. \"\n",
        "        \"Rules:\\n\"\n",
        "        \"- 'technical': bugs, errors, code issues, crashes\\n\"\n",
        "        \"- 'billing': charges, refunds, subscriptions, payments\\n\"\n",
        "        \"- 'tool': requests for live/real-time data like cryptocurrency prices, stock prices, weather\\n\"\n",
        "        \"- 'general': everything else\\n\"\n",
        "        \"Return ONLY the single category word in lowercase. No punctuation, no explanation.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": routing_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=10,\n",
        "    )\n",
        "\n",
        "    category = response.choices[0].message.content.strip().lower()\n",
        "    if category not in list(MODEL_CONFIG.keys()) + [\"tool\"]:\n",
        "        category = \"general\"\n",
        "    return category\n",
        "\n",
        "\n",
        "def process_request_with_tool(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Extended orchestrator that handles the 'tool' expert by calling\n",
        "    a mock data-fetching function instead of an LLM.\n",
        "    \"\"\"\n",
        "    category = route_prompt_with_tool(user_input)\n",
        "    print(f\"[Router] Classified as: '{category}'\")\n",
        "\n",
        "    # Handle tool use separately — call the function, not the LLM\n",
        "    if category == \"tool\":\n",
        "        if \"bitcoin\" in user_input.lower():\n",
        "            return get_mock_bitcoin_price()\n",
        "        return \"Tool use requested, but no matching tool was found.\"\n",
        "\n",
        "    # Otherwise, use the normal expert pipeline\n",
        "    expert_config = MODEL_CONFIG[category]\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": expert_config[\"system_prompt\"]},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        temperature=expert_config[\"temperature\"],\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "# Test the Tool Use Expert\n",
        "query_4 = \"What is the current price of Bitcoin?\"\n",
        "print(\"=\" * 60)\n",
        "print(f\"User Query: {query_4}\")\n",
        "print(\"=\" * 60)\n",
        "answer_4 = process_request_with_tool(query_4)\n",
        "print(f\"\\n[Expert Response]\\n{answer_4}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}