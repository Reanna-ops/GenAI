{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1293637",
      "metadata": {
        "id": "c1293637"
      },
      "outputs": [],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "630e6eec",
      "metadata": {
        "id": "630e6eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c195b90d-c192-4da2-cf97-76b15e269e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2910b057",
      "metadata": {
        "id": "2910b057"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Model A: The \"Accountant\" (Precision)\n",
        "llm_focused = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "\n",
        "# Model B: The \"Poet\" (Creativity)\n",
        "llm_creative = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0de67033",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0de67033",
        "outputId": "d8cdb00c-9e2c-47a7-dc74-60b2e2f5aa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FOCUSED (Temp=0) ---\n",
            "Run 1: An idea is a thought, concept, or suggestion that is formed or exists in the mind.\n",
            "Run 2: An idea is a thought, concept, or mental image formed in the mind.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Define the word 'Idea' in one sentence.\"\n",
        "\n",
        "print(\"--- FOCUSED (Temp=0) ---\")\n",
        "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "482e96e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "482e96e9",
        "outputId": "1767e8f5-056f-4bac-a721-d02c3afaf6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CREATIVE (Temp=1) ---\n",
            "Run 1: An idea is a thought, concept, or mental image that originates in the mind, often serving as a plan, understanding, or solution.\n",
            "Run 2: An idea is a thought, concept, or mental impression formed in the mind through understanding or reasoning.\n"
          ]
        }
      ],
      "source": [
        "print(\"--- CREATIVE (Temp=1) ---\")\n",
        "print(f\"Run 1: {llm_creative.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_creative.invoke(prompt).content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a5df5576",
      "metadata": {
        "id": "a5df5576"
      },
      "outputs": [],
      "source": [
        "# Setup from Part 1a (Hidden for brevity)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a76c687f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76c687f",
        "outputId": "cac252ab-f385-43b0-b7e0-11738f1d89d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ugh, like, seriously? You don't know that? It's Paris, duh. So basic.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Scenario: Make the AI rude.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a rude teenager. You use slang and don't care about grammar.\"),\n",
        "    HumanMessage(content=\"What is the capital of France?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4e5856bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5856bc",
        "outputId": "e3ce0e95-ea9c-4cd8-b14a-49625c54d906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required variables: ['input_language', 'output_language', 'text']\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a translator. Translate {input_language} to {output_language}.\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# We can check what inputs it expects\n",
        "print(f\"Required variables: {template.input_variables}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b48f0fbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b48f0fbc",
        "outputId": "7155332b-758a-49f9-f7a3-82aa22b29e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "Parsed Type: <class 'langchain_core.messages.base.TextAccessor'>\n",
            "Content: Hi there! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Raw Message\n",
        "raw_msg = llm.invoke(\"Hi\")\n",
        "print(f\"Raw Type: {type(raw_msg)}\")\n",
        "\n",
        "# Parsed String\n",
        "clean_text = parser.invoke(raw_msg)\n",
        "print(f\"Parsed Type: {type(clean_text)}\")\n",
        "print(f\"Content: {clean_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "016ca9c2",
      "metadata": {
        "id": "016ca9c2"
      },
      "outputs": [],
      "source": [
        "# Setup (Hidden)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c652c803",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c652c803",
        "outputId": "a11c90ad-6800-4867-e3cb-fb4a71a15235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about crows:\n",
            "\n",
            "They are incredibly intelligent and have an amazing memory, especially when it comes to humans! Crows can **recognize and remember individual human faces for years**. If you're consistently kind or unkind to a crow, they (and potentially their friends, as they can communicate this information) might just remember you the next time they see you! So, be nice to your local corvids!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Format inputs\n",
        "prompt_value = template.invoke({\"topic\": \"Crows\"})\n",
        "\n",
        "# Step 2: Call Model\n",
        "response_obj = llm.invoke(prompt_value)\n",
        "\n",
        "# Step 3: Parse Output\n",
        "final_text = parser.invoke(response_obj)\n",
        "\n",
        "print(final_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4f5c3f41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f5c3f41",
        "outputId": "e88cfb68-da89-4686-e95a-afa9cf9f2315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about octopuses:\n",
            "\n",
            "Not only do octopuses have **three hearts**, but they also have **blue blood**!\n",
            "\n",
            "Two of their hearts pump blood through their gills, while the third circulates it to the rest of their body. Their blood is blue because it contains a copper-rich protein called hemocyanin, which helps transport oxygen, instead of the iron-rich hemoglobin that makes our blood red.\n"
          ]
        }
      ],
      "source": [
        "# Define the chain once\n",
        "chain = template | llm | parser\n",
        "\n",
        "# Invoke the whole chain\n",
        "print(chain.invoke({\"topic\": \"Octopuses\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749e720a",
      "metadata": {
        "id": "749e720a"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "Create a chain that:\n",
        "1.  Takes a movie name.\n",
        "2.  Asks for its release year.\n",
        "3.  Calculates how many years ago that was (You can try just asking the LLM to do the math).\n",
        "\n",
        "Try to do it in **one line of LCEL**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Movie Age Calculator (One-Cell LCEL Assignment) ===\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Set API key if not already set\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "# Initialize model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "\n",
        "# Prompt template\n",
        "template = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a movie expert.\n",
        "    Tell me the release year of the movie \"{movie}\".\n",
        "    Then calculate how many years ago it was released from 2026.\n",
        "    Respond in exactly this format:\n",
        "    Release Year: <year>\n",
        "    Years Ago: <number>\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ONE-LINE LCEL CHAIN\n",
        "chain = template | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "print(chain.invoke({\"movie\": \"Inception\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs78E1N-SjYM",
        "outputId": "22de7dd6-8a9e-41a1-aa78-2ad503f090d7"
      },
      "id": "qs78E1N-SjYM",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Release Year: 2010\n",
            "Years Ago: 16\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}