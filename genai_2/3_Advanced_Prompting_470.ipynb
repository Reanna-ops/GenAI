{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba92b198",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba92b198",
        "outputId": "ab2ce127-bf7f-479d-a1af-c99779806727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m102.4/111.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3088780",
      "metadata": {
        "id": "e3088780"
      },
      "source": [
        "## 3. The Experiment: A Tricky Math Problem\n",
        "\n",
        "Let's try a problem that requires multi-step logic.\n",
        "\n",
        "**Problem:**\n",
        "\"Roger has 10 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a70d3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a70d3b7",
        "outputId": "5a6d467b-a753-4f22-c5d6-73981d1dfc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (10) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (10) to the number of tennis balls he bought (6):\n",
            "\n",
            "10 + 6 = 16\n",
            "\n",
            "So, Roger now has 16 tennis balls.\n"
          ]
        }
      ],
      "source": [
        "question = \"Roger has 10 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3dd65b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd65b0a",
        "outputId": "56f67b46-bd8c-4cb8-c89f-a3b50b1e58e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
            "\n",
            "1. Roger starts with 10 tennis balls.\n",
            "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 tennis balls.\n",
            "3. To find the total number of tennis balls Roger has now, we add the initial number of tennis balls (10) to the number of tennis balls he bought (6). \n",
            "\n",
            "10 (initial tennis balls) + 6 (new tennis balls) = 16\n",
            "\n",
            "So, Roger now has 16 tennis balls.\n"
          ]
        }
      ],
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4371aa3d",
      "metadata": {
        "id": "4371aa3d"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1ea2d4c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea2d4c7",
        "outputId": "6b07b7f3-ac14-424b-9997-5a87e679740f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "As a business woman, I have carefully evaluated the three proposed solutions and I am excited to share that I believe the most sustainable one is **The \"Future-Proofing\" Billionaire Strategy: Invest in and Develop Revolutionary, Sustainable, and In-Demand Technologies**.\n",
            "\n",
            "Here's why I think this solution stands out:\n",
            "\n",
            "1. **Long-term impact**: By focusing on sustainable and in-demand technologies, this solution has the potential to create a lasting impact on the environment and society, which aligns with my values as a business woman.\n",
            "2. **Scalability**: The solution highlights the importance of designing and developing technologies that can be easily scaled up to meet growing demand, which is crucial for long-term success and sustainability.\n",
            "3. **Diversified revenue streams**: By investing in various technologies, such as renewable energy, sustainable food production, and AI for social good, this solution creates multiple revenue streams and reduces dependence on a single market or industry.\n",
            "4. **Talent attraction and retention**: The solution emphasizes the importance of assembling a dream team of top talent from various fields, which is essential for attracting and retaining the best minds in the industry.\n",
            "5. **Continuous innovation**: By staying ahead of the curve and continuously monitoring market trends, customer needs, and technological advancements, this solution ensures that the business remains agile and adaptable, which is critical for long-term success in a rapidly changing world.\n",
            "\n",
            "In contrast, while the other two solutions, **Create a Virtual Reality Experience that Changes the World** and **Create a Virtual Reality Utopia**, have the potential to be innovative and impactful, they also come with significant challenges and limitations. For example, the virtual reality industry is still in its early stages, and there are concerns about the potential negative impacts of excessive screen time and social isolation.\n",
            "\n",
            "**Create a Virtual Reality Experience that Changes the World**, while it has the potential to raise awareness about important social and environmental issues, also raises questions about the effectiveness of virtual reality in creating lasting change. Additionally, the solution relies heavily on partnerships with influential organizations, which can be uncertain and unreliable.\n",
            "\n",
            "**Create a Virtual Reality Utopia**, while it has the potential to create a unique and engaging experience, also raises significant challenges related to the design and implementation of a sustainable economic model, social structure, and marketing strategy.\n",
            "\n",
            "In conclusion, while all three solutions have potential, **The \"Future-Proofing\" Billionaire Strategy: Invest in and Develop Revolutionary, Sustainable, and In-Demand Technologies** stands out as the most sustainable and viable option for creating a successful and impactful billion-dollar business that aligns with my values as a business woman.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I be a successful billionaire?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Business Woman. Pick the most sustainable one (not bribery) and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "894940b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894940b5",
        "outputId": "668492d8-4acd-468f-fc2c-6ac65c4579b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "In \"Echoes of Eternity,\" a brilliant but reclusive physicist, Emma Taylor, discovers a way to manipulate time and traverse the fabric of space-time. As she experiments with her creation, a mysterious entity from a dystopian future begins to seep into her reality, threatening to destroy the world. Desperate to prevent this catastrophic future, Emma becomes obsessed with her invention, using it to travel back in time and find the source of the disturbance. However, her journey takes an unexpected turn when she encounters a charming and enigmatic artist, Max, who is living in the past. As Emma navigates the complexities of time travel and her growing feelings for Max, she must confront the dark forces manipulating her reality and the terrible consequences of altering the timeline, forcing her to make an impossible choice between saving the world and losing the love of her life.\n"
          ]
        }
      ],
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455cb724",
      "metadata": {
        "id": "455cb724"
      },
      "source": [
        "## 4. Summary & Comparison Table\n",
        "\n",
        "| Method | Structure | Best For... | Cost/Latency |\n",
        "|--------|-----------|-------------|--------------|\n",
        "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
        "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
        "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High |\n",
        "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
        "\n",
        "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}