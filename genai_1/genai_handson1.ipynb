{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee09943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Reanna Netto\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bb9317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: click in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\reanna netto\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Reanna Netto\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e349564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\reanna netto\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\reanna netto\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (26.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\reanna netto\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.12.19)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.6)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.46.3)\n",
      "Requirement already satisfied: namex in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: rich in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2026.1.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.6.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (12.1.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\reanna netto\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\reanna netto\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Reanna Netto\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3797ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c465d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"C:\\Users\\Reanna Netto\\unit 1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7b8165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text=f.read()\n",
    "    print(\"File loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"error: '(file_path)' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ca3bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview\n",
      "Generative AI and Its Applications: A Foundational Briefing\n",
      "\n",
      "Executive Summary\n",
      "\n",
      "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preview\")\n",
    "print(text[:500]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43fe9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7750a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Gen AI is a powerful tech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c20b34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 265.12it/s, Materializing param=transformer.wte.weight]            \n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Passing `generation_config` together with generation-related arguments=({'num_return_sequences', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=43) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen AI is a powerful tech that was invented by the then-President Bill Clinton.\n",
      "\n",
      "\n",
      "\n",
      "The AI technology was invented by the then-President Bill Clinton.\n",
      "In 2004, Bill Clinton was speaking at a conference hosted by the California\n"
     ]
    }
   ],
   "source": [
    "fast_generator= pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "output_fast=fast_generator(prompt, max_new_tokens=43, num_return_sequences=3)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565a401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 231.58it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=43) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen AI is a powerful tech that can solve many of the problems that AI has faced. The reason that this tech is being developed is because it has the potential to save lives in humans by allowing them to do so without having to worry about the\n"
     ]
    }
   ],
   "source": [
    "fast_generator= pipeline('text-generation', model='gpt2')\n",
    "\n",
    "output_fast=fast_generator(prompt, max_new_tokens=43, num_return_sequences=3)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f31580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb27bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence='Transformers revolutionized NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2bd43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Transform', 'ers', 'Ġrevolution', 'ized', 'ĠN', 'LP']\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenizer.tokenize(sample_sentence)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b72cc10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens id: [41762, 364, 5854, 1143, 399, 19930]\n"
     ]
    }
   ],
   "source": [
    "token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Tokens id: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e03c56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159a80f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Reanna\n",
      "[nltk_data]     Netto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99503929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
    "print(f\"POS Tags: {pos_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b08f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 391/391 [00:01<00:00, 244.62it/s, Materializing param=classifier.weight]                                      \n",
      "BertForTokenClassification LOAD REPORT from: dbmdz/bert-large-cased-finetuned-conll03-english\n",
      "Key                      | Status     |  | \n",
      "-------------------------+------------+--+-\n",
      "bert.pooler.dense.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "#Initialize NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab36a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity               | Type       | Score\n",
      "---------------------------------------------\n",
      "AI                   | MISC       | 0.98\n",
      "PES University       | ORG        | 0.99\n",
      "AI                   | MISC       | 0.98\n",
      "Large Language Models | MISC       | 0.91\n",
      "LLMs                 | MISC       | 0.90\n",
      "Transformer          | MISC       | 0.99\n"
     ]
    }
   ],
   "source": [
    "snippet = text[:1000]\n",
    "entities = ner_pipeline(snippet)\n",
    "\n",
    "print(f\"{'Entity':<20} | {'Type':<10} | {'Score':<5}\")\n",
    "print(\"-\"*45)\n",
    "for entity in entities:\n",
    "    if entity['score'] > 0.90:\n",
    "        print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f207825",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_section = \"\"\"\n",
    "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions.\n",
    "The fundamental innovation of the Transformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the input, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.\n",
    "The Transformer architecture consists of an encoder stack (to process the input) and a decoder stack (to generate the output), both of which heavily utilize multi-head attention and feed-forward networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ab41ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Please make sure the generation config includes `forced_bos_token_id=0`. \n",
      "Loading weights: 100%|██████████| 358/358 [00:01<00:00, 275.14it/s, Materializing param=model.shared.weight]                                  \n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI . It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and conv\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"  # or facebook/bart-large-cnn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    transformer_section,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=60,\n",
    "    min_length=30,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88885706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 511/511 [00:02<00:00, 242.14it/s, Materializing param=model.encoder.layers.11.self_attn_layer_norm.weight]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    transformer_section,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=60,\n",
    "    min_length=30,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b9c7711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 102/102 [00:00<00:00, 260.39it/s, Materializing param=qa_outputs.weight]                                     \n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f824c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is the fundamental innovation of the Transformer?\n",
      "A: to identify hidden patterns, structures, and relationships within the data\n",
      "\n",
      "Q: What are the risks of using Generative AI?\n",
      "A: data privacy, intellectual property, and academic integrity\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the fundamental innovation of the Transformer?\",\n",
    "    \"What are the risks of using Generative AI?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=text[:5000])\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "412f52e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 287.36it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "934701e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applications: 0.06\n",
      "ideas: 0.05\n",
      "problems: 0.05\n",
      "systems: 0.04\n",
      "information: 0.03\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
